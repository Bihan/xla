apiVersion: v1
kind: Pod
metadata:
  generateName: xla-test-job-
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: tpu.googleapis.com/type
            operator: In
            values:
            - v2-8
  restartPolicy: Never
  volumes:
  # Increase size of tmpfs /dev/shm to avoid OOM.
  - name: dshm
    emptyDir:
      medium: Memory
  containers:
  - name: xla-test
    securityContext:
      privileged: true
    image: gcr.io/$PROJECT_ID/pytorch-xla-test:$BUILD_ID
    command:
    - bash
    - -cxu
    - |
      python3 pytorch/xla/test/test_operations.py -v
      python3 pytorch/xla/test/pjrt/test_experimental_pjrt_tpu.py
      python3 pytorch/xla/test/spmd/test_xla_sharding.py
      python3 pytorch/xla/test/spmd/test_xla_virtual_device.py
      python3 pytorch/xla/test/spmd/test_train_spmd_linear_model.py

      TPU_LIBRARY_PATH=$(dirname $(python -c 'import torch_xla; print(torch_xla.__file__)'))/lib/libtpu.so $(python -c 'import sys; print(sys.prefix)')/test_ptxla
    volumeMounts:
    - mountPath: /dev/shm
      name: dshm
    env:
    - name: PJRT_DEVICE
      value: TPU
